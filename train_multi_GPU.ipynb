{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed9fd5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 16:58:02,476 INFO     pid:41854 __main__:078:initModel Using CUDA; 3 devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end_to_end_Net\n",
      "clear_tof_amp_npys size:3000\n",
      "clear_tof_pha_npys size:3000\n",
      "fog_tof_amp_npys_path size:3000\n",
      "fog_tof_pha_npys_path size:3000\n",
      "clear_tof_amp_npys size:500\n",
      "clear_tof_pha_npys size:500\n",
      "fog_tof_amp_npys_path size:500\n",
      "fog_tof_pha_npys_path size:500\n",
      "outside:input size: torch.Size([72, 1, 256, 256])\n",
      "image1: tensor(-248.2685+40.7867j, device='cuda:0')\n",
      "image2: tensor(-248.6225+42.4000j, device='cuda:0')\n",
      "new_x torch.Size([72, 1, 256, 256])\n",
      "new_x1 tensor(-248.2685+40.7867j, device='cuda:0')\n",
      "new_x2 tensor(-248.6225+42.4000j, device='cuda:0')\n",
      "\tIn Model: input size torch.Size([72, 1, 256, 256]) output size torch.Size([72, 1, 256, 256])\n",
      "outside:output size: torch.Size([72, 1, 256, 256])\n",
      "Epoch: [1]  [ 0/42]  eta: 0:04:30  lr: 0.000002  loss: 1472.5143 (1472.5143)  time: 6.4368  data: 3.1725  max mem: 27011\n",
      "outside:input size: torch.Size([72, 1, 256, 256])\n",
      "image1: tensor(43.4663+249.0388j, device='cuda:0')\n",
      "image2: tensor(43.4504+248.9893j, device='cuda:0')\n",
      "new_x torch.Size([72, 1, 256, 256])\n",
      "new_x1 tensor(43.4663+249.0388j, device='cuda:0')\n",
      "new_x2 tensor(43.4504+248.9893j, device='cuda:0')\n",
      "\tIn Model: input size torch.Size([72, 1, 256, 256]) output size torch.Size([72, 1, 256, 256])\n",
      "outside:output size: torch.Size([72, 1, 256, 256])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 202\u001b[0m\n\u001b[1;32m    185\u001b[0m         os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./save_weights\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m#     parser = argparse.ArgumentParser(description=__doc__)\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m#     parser.add_argument('--data-path', default='./data', help='dataset')\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m#     parser.add_argument('--device', default='cuda', help='device')\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m#     args = parser.parse_args(args=['--data-path', './data'])\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m     \u001b[43mTrainingApp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 158\u001b[0m, in \u001b[0;36mTrainingApp.main\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler \u001b[38;5;241m=\u001b[39m create_lr_scheduler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, num_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_DL), epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs, warmup\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_ndx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 158\u001b[0m     train_loss, lr \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_DL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_ndx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m evaluate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, val_DL, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    162\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "File \u001b[0;32m/workspace/ToFDeFUnet/train_and_eval.py:80\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch, lr_scheduler, scaler)\u001b[0m\n\u001b[1;32m     78\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m     \u001b[43mtrain_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     83\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:491\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    483\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    484\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    489\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    490\u001b[0m     )\n\u001b[0;32m--> 491\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import torch.utils.data.dataloader as DataLoader\n",
    "import sys\n",
    "import argparse\n",
    "import transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from model import SimpleNet\n",
    "from Src.ComplexValuedAutoencoderMain_Torch import end_to_end_Net\n",
    "from dataset import ToFDataset\n",
    "from train_and_eval import train_one_epoch, evaluate, create_lr_scheduler\n",
    "from util.logconf import logging\n",
    "\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "# log.setLevel(logging.WARN)\n",
    "log.setLevel(logging.INFO)\n",
    "log.setLevel(logging.DEBUG)\n",
    "\n",
    "class PresetTrain:\n",
    "    def __init__(self, crop_size, hflip_prob=0.5):\n",
    "        \n",
    "        trans = []\n",
    "        if hflip_prob > 0:\n",
    "            trans.append(T.RandomHorizontalFlip(hflip_prob))\n",
    "        trans.extend([\n",
    "            T.RandomCrop(crop_size),\n",
    "            \n",
    "        ])\n",
    "        self.transforms = T.Compose(trans)\n",
    "\n",
    "    def __call__(self, img, target):\n",
    "        return self.transforms(img, target)\n",
    "\n",
    "\n",
    "class PresetEval:\n",
    "    def __init__(self, crop_size):\n",
    "        trans = []\n",
    "        trans.extend([\n",
    "            T.CenterCrop(crop_size),\n",
    "#             T.RandomCrop(crop_size),\n",
    "        ])\n",
    "        self.transforms = T.Compose(trans)\n",
    "\n",
    "    def __call__(self, img, target):\n",
    "        return self.transforms(img, target)\n",
    "\n",
    "def get_transform(train):\n",
    "    crop_size = 256\n",
    "\n",
    "    if train:\n",
    "        return PresetTrain(crop_size)\n",
    "    else:\n",
    "        return PresetEval(crop_size)\n",
    "\n",
    "class TrainingApp:\n",
    "    def __init__(self, args=None):\n",
    "    \n",
    "        self.lr = 0.0001\n",
    "        self.path = './data'\n",
    "        self.batch_size = 72\n",
    "        self.epochs = 200\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        self.device = torch.device('cuda' if self.use_cuda else 'cpu')\n",
    "        self.num_workers = 8\n",
    "        self.model = self.initModel()\n",
    "        self.optimizer = self.initOptimizer(self.lr)\n",
    "       \n",
    "      \n",
    "\n",
    "    def initModel(self):\n",
    "        model = end_to_end_Net(1,1,1,bilinear=True)\n",
    "        if self.use_cuda:\n",
    "            log.info(\"Using CUDA; {} devices.\".format(\n",
    "                torch.cuda.device_count()))\n",
    "#             if torch.cuda.device_count() > 1:\n",
    "#                 model = torch.nn.DataParallel(model, device_ids=[0,1,2])\n",
    "            model = model.to(self.device)\n",
    "        return model\n",
    "\n",
    "    def initOptimizer(self, lr):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr)\n",
    "        return optimizer\n",
    "    \n",
    "\n",
    "    def initTrainDL(self):\n",
    "        train_dataset = ToFDataset(self.path, train=True, transforms=get_transform(train=True))\n",
    "#         if self.distributed:\n",
    "#             train_sampler = data.distributed.DistributedSampler(train_dataset)\n",
    "#         else:\n",
    "#             train_sampler = data.RandomSampler(train_dataset)\n",
    "        \n",
    "#         if self.use_cuda:\n",
    "#             batch_size *= torch.cuda.device_count()\n",
    "\n",
    "        train_DL = torch.utils.data.DataLoader(train_dataset,\n",
    "                              batch_size=self.batch_size, \n",
    "                              shuffle=True,\n",
    "                              num_workers=self.num_workers,\n",
    "                              pin_memory=True,)\n",
    "        return train_DL\n",
    "\n",
    "    def initValDL(self):\n",
    "        val_dataset = ToFDataset(self.path, train=False, transforms=get_transform(train=False))\n",
    "#         if self.distributed:\n",
    "#             val_sampler = data.distributed.DistributedSampler(val_dataset)\n",
    "#         else:\n",
    "#             val_sampler = data.RandomSampler(val_dataset)\n",
    "\n",
    "#         if self.use_cuda:\n",
    "#             batch_size *= torch.cuda.device_count()\n",
    "\n",
    "        val_DL = torch.utils.data.DataLoader(val_dataset,\n",
    "                            batch_size=self.batch_size,\n",
    "                            num_workers=self.num_workers,\n",
    "                            pin_memory=self.use_cuda,\n",
    "                            shuffle=False)\n",
    "        return val_DL\n",
    "    \n",
    "\n",
    "    def showPlt(self, train_losses, val_losses):\n",
    "        epochs = np.arange(1, self.epochs+1)\n",
    "        plt.figure()\n",
    "        plt.plot(epochs, train_losses, label='train_losses')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('train_losses')\n",
    "        plt.title('Training Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(epochs, val_losses, label='val_losses')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('val_losses')\n",
    "        plt.title('Validation Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.show()    \n",
    "        \n",
    "    def main(self):\n",
    "        # log.info(\"Starting {}, {}\".format(type(self).__name__, self.cli_args))\n",
    "        results_file = \"results{}.txt\".format(datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "        \n",
    "        train_DL = self.initTrainDL()\n",
    "        val_DL = self.initValDL()\n",
    "        min_loss = 10000\n",
    "\n",
    "        start_time = time.time()\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        self.lr_scheduler = create_lr_scheduler(self.optimizer, num_step=len(train_DL), epochs=self.epochs, warmup=True)\n",
    "        \n",
    "        for epoch_ndx in range(1, self.epochs + 1):\n",
    "                \n",
    "            train_loss, lr = train_one_epoch(self.model, self.optimizer, train_DL, self.device, epoch_ndx, self.lr_scheduler, scaler=None)\n",
    "            \n",
    "            val_loss = evaluate(self.model, val_DL, self.device)\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "            save_file = {\"model\": self.model.state_dict(),\n",
    "                        \"optimizer\": self.optimizer.state_dict(),\n",
    "                        \"lr_scheduler\": self.lr_scheduler.state_dict(),\n",
    "                        \"epoch\": epoch_ndx,\n",
    "                        # \"args\": args\n",
    "                        }\n",
    "            \n",
    "            if val_loss < min_loss:\n",
    "                min_loss = val_loss\n",
    "                torch.save(save_file, \"save_weights/best_model.pth\")\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "        print(\"Training time {}\".format(total_time_str))\n",
    "        self.showPlt(train_losses, val_losses)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "    if not os.path.exists('./save_weights'):\n",
    "        os.makedirs('./save_weights')\n",
    "        \n",
    "#     parser = argparse.ArgumentParser(description=__doc__)\n",
    "#     parser.add_argument('--data-path', default='./data', help='dataset')\n",
    "#     parser.add_argument('--device', default='cuda', help='device')\n",
    "#     parser.add_argument('-b', '--batch-size', default=4, type=int, help='images per gpu, the total batch size is $NGPU x batch_size')\n",
    "#     parser.add_argument('--epochs', default=200, type=int, metavar='N', help='number of total epochs to run')\n",
    "#     parser.add_argument('--sync_bn', type=bool, default=False, help='whether using SyncBatchNorm')\n",
    "#     parser.add_argument('-j', '--workers', default=4, type=int, metavar='N', help='number of data loading workers (default: 4)')\n",
    "#     parser.add_argument('--lr', default=0.01, type=float, help='initial learning rate, 0.01*GPU number')\n",
    "#     parser.add_argument('--output-dir', default='./multi_train', help='path where to save')\n",
    "    \n",
    "#     parser.add_argument('--world-size', default=1, type=int, help='number of distributed processes')\n",
    "#     parser.add_argument('--dist-url', default='env://', help='url used to set up distributed training')\n",
    "    \n",
    "#     args = parser.parse_args(args=['--data-path', './data'])\n",
    "    \n",
    "    TrainingApp().main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb11229f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8739212",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
